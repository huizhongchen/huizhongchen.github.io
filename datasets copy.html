<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" />
<title> Datasets - Huizhong Chen </title>
<link rel="stylesheet" type="text/css" href="main.css" media="all " />
</head> 

<body class="datasets">
<p class="skip"><a href="#content">Skip to content</a></p>
<div id="sitename">
<a href="http://www.stanford.edu"><img src="imgs/stanford_med.gif" alt="Stanford University" /></a>
</div>

  <div id="header">
    <div><div>
      <H2>Huizhong Chen</H2>
           
      <hr class="clearer" />

    </div></div>
  </div>




<div id="container">


    <div id="sidebar">
      <a href="#"><img src="imgs/Packard_Back.jpg" alt="Packard" id="bannerimg" width="160" height="112" /></a>
        <div id="nav"> 
        <ul id="navmenu">
        <li id="home">
	      <a href="index.html">Home</a>
	    </li>
	    <li id="publications">
	      <a href="publications.html">Publications</a>
	    </li>
        <li id="datasets">
            <a href="datasets.html">Datasets</a>
	    </li>
        <li id="misc">
	      <a href="misc.html">Misc</a>
	    </li>
		<li id="resume">
		  <a href="http://www.stanford.edu/~hchen2/CV_HuizhongChen.pdf">Resume</a>
		</li>
        <li id="ivms_group">
	      <a href="http://www.stanford.edu/~bgirod/People.html">IVMS Group</a>
	    </li>
          </ul>
        </div>
        <p>
	  <br/>
	  <img src="imgs/seal_160.gif" width="159" height="140" alt="Go to Stanford Homepage" /><br/><br/>
	</p>
	
    </div>

	    

<div id="content">
<!-- TemplateBeginEditable name="ContentArea" -->

<a name="top"></a>
<center>
<h2>
Datasets
</h2>
</center>
<br/>

<ul>
<li>
<a href="#googleiodataset">Google I/O Dataset</a><br/>
</li>
<li>
<a href="#names100dataset">Names 100 Dataset</a><br/>
</li>
<li>
<a href="#clothingattributedataset">Clothing Attributes Dataset</a><br/>
</li>
<li>
<a href="#mvsdataset">Stanford Mobile Visual Search Dataset</a><br/>
</li>
<li>
<a href="#cnn2hvideosdataset">CNN 2-Hours Videos Dataset</a><br/>
</li>
</ul>
<br><br>


<a name="googleiodataset"></a>
<hr />
<br />

<h3>Google I/O Dataset</h3><br/>
The Google I/O Dataset contains slide and spoken text data crawled from 209 presentations in the Google I/O Conference (2010-2012), with 275 manually labeled ground truth relevance judgements. The dataset is particularly suitable for studying information retrieval using multi-modal data. 
<br /><br />

<a href="http://purl.stanford.edu/gc512qf7480"><b>Download Dataset</b></a>
<br/><br/>

<center>
    <img src="imgs/GoogleIO_Dataset.jpg" height="285"><br><br><br>
</center>

References:<br/>
<ol>
    <li>
        H. Chen, M. Cooper, D. Joshi, and B. Girod, "Multi-modal Language Models for Lecture Video Retrieval", ACM Multimedia (MM), October 2014. [<a href="papers/ACM_MM14_multi-modal-language.pdf">Paper</a>]
    </li>
</ol>
<br>

<a href="#top">Back to Top</a>
<br/><br/><br/>

<a name="names100dataset"></a>
<hr />
<br />

<h3>Names 100 Dataset</h3><br/>
The Google I/O Dataset contains slide and spoken text data crawled from 209 presentations in the Google I/O Conference (2010-2012), with 275 manually labeled ground truth relevance judgements. The dataset is particularly suitable for studying information retrieval using multi-modal data. 
<br /><br />

<a href="http://purl.stanford.edu/tp945cq9122"><b>Download Dataset</b></a>
<br/><br/>

<center>
    <img src="imgs/Names100_Dataset.jpg" height="350"><br><br><br>
</center>

References:<br/>
<ol>
    <li>
		Huizhong Chen, Andrew Gallagher, and Bernd Girod,  
		"What's in a Name: First Names as Facial Attributes",
		IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2013.
         [<a href="papers/CVPR2013_NamesAsAttributes.pdf">Paper</a>]
    </li>
	<li>
	Huizhong Chen, Andrew Gallagher, and Bernd Girod,
	"The Hidden Sides of Names - Face Modeling with First Name Attributes", 
	IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014.
	[<a href="papers/PAMI_NameAttributes_Chen_Gallagher_Girod.pdf">Paper</a>]
	</li>
</ol>
<br>

<a href="#top">Back to Top</a>
<br/><br/><br/>



<a name="clothingattributedataset"></a>
<hr />
<br />

<h3>Clothing Attributes Dataset</h3><br/>
We introduce the Clothing Attributes Dataset for promoting research in learning visual attributes for objects. The dataset contains 1856 images, with 26 ground truth clothing attributes such as "long-sleeves", "has collar", and "striped pattern". The labels were collected using Amazon Mechanical Turk.
<br /><br />

<a href="http://purl.stanford.edu/tb980qz1002"><b>Download Dataset</b></a>
<br/><br /><br/>

<center>
    <img src="imgs/Clothing_Attributes_Dataset.jpg" height="215"><br><br><br>
</center>

References:<br/>
<ol>
    <li>
        H. Chen, A. Gallagher, and B. Girod, "Describing Clothing by Semantic Attributes", European Conference on Computer Vision (ECCV), October 2012. [<a href="papers/ECCV2012_ClothingAttributes.pdf">Paper</a>]
    </li>
</ol>
<br>


<a href="#top">Back to Top</a>
<br/><br/>


<a name="mvsdataset"></a>
<hr />
<br />

<h3>Stanford Mobile Visual Search Dataset</h3><br/>
We propose the Stanford Mobile Visual Search dataset. 
The dataset contains camera-phone images of products, CDs, books, outdoor landmarks, business cards, text documents, museum paintings and video clips. 
The dataset has several key characteristics lacking in existing datasets: rigid objects, widely varying lighting conditions, perspective distortion, foreground and background clutter, realistic ground-truth reference data, and query data collected from heterogeneous low and high-end camera phones. 
We hope that the dataset will help push research forward in the field of mobile visual search.
<br /><br />

<a href="http://purl.stanford.edu/rb470rw0983"><b>Download Dataset</b></a>
<br/><br /><br />

<table width="500" cellpadding="0" cellspacing="0" border="0">
<tr valign="middle">
<td width="100">
<center>
<img src="imgs/MVS_Dataset_Book_Reference_001.jpg" width="90" height="139" border="0" />
<br />
Reference
</center>
</td>
<td width="100">
<center>
<img src="imgs/MVS_Dataset_Book_Droid_001.jpg" width="100" height="75" border="0" />
<br />
Motorola Droid
</center>
</td>
<td width="100">
<center>
<img src="imgs/MVS_Dataset_Book_5800_001.jpg" width="100" height="75" border="0" />
<br />
Nokia 5800
</center>
</td>
<td width="100">
<center>
<img src="imgs/MVS_Dataset_Book_iPhone_001.jpg" width="100" height="75" border="0" />
<br />
Apple iPhone
</center>
</td>
<td width="100">
<center>
<img src="imgs/MVS_Dataset_Book_Canon_001.jpg" width="100" height="75" border="0" />
<br />
Canon G11
</center>
</td>
</tr>
<tr valign="middle">
<td width="100">
<center>
<img src="imgs/MVS_Dataset_DVD_Reference_003.jpg" width="120" height="120" border="0" />
<br />
Reference
</center>
</td>
<td width="100">
<center>
<img src="imgs/MVS_Dataset_DVD_Droid_003.jpg" width="100" height="75" border="0" />
<br />
Motorola Droid
</center>
</td>
<td width="100">
<center>
<img src="imgs/MVS_Dataset_DVD_Pre_003.jpg" width="100" height="75" border="0" />
<br />
Palm Pre
</center>
</td>
<td width="100">
<center>
<img src="imgs/MVS_Dataset_DVD_E63_003.jpg" width="100" height="75" border="0" />
<br />
Nokia E63
</center>
</td>
<td width="100">
<center>
<img src="imgs/MVS_Dataset_DVD_Canon_003.jpg" width="100" height="75" border="0" />
<br />
Canon G11
</center>
</td>
</tr>
<tr valign="middle">
<td width="100">
<center>
<img src="imgs/MVS_Dataset_Painting_Reference_017.jpg" width="80" height="150" border="0" />
<br />
Reference
</center>
</td>
<td width="100">
<center>
<img src="imgs/MVS_Dataset_Painting_Droid_017.jpg" width="100" height="75" border="0" />
<br />
Motorola Droid
</center>
</td>
<td width="100">
<center>
<img src="imgs/MVS_Dataset_Painting_Pre_017.jpg" width="75" height="100" border="0" />
<br />
Palm Pre
</center>
</td>
<td width="100">
<center>
<img src="imgs/MVS_Dataset_Painting_E63_017.jpg" width="100" height="75" border="0" />
<br />
Nokia E63
</center>
</td>
<td width="120">
<center>
<img src="imgs/MVS_Dataset_Painting_Canon_017.jpg" width="100" height="75" border="0" />
<br />
Canon G11
</center>
</td>
</tr>
</table>
<br /><br/>

References:<br/>
<ol>
<li>
V. Chandrasekhar, D. Chen, S. Tsai, N.-M. Cheung, H. Chen, G. Takacs, Y. Reznik, R. Vedantham, R. Grzeszczuk, J. Bach, and B. Girod, "The Stanford mobile visual search dataset", ACM Multimedia Systems Conference (MMSys), February 2011.
[<a href="papers/ACMMMSys2011_VisualSearchDataset.pdf">Paper</a>]
</li>
</ol>
<br/>

<a href="#top">Back to Top</a>
<br/><br/>


<a name="cnn2hvideosdataset"></a>
<hr />
<br />

<h3>CNN 2-Hours Videos Dataset</h3><br/>

We present the CNN2h dataset, which can be used for evaluating systems that search videos using image queries. It contains 2 hours of video and 139 image queries with annotated ground truth (based on video frames extracted at 10 frames per second). The annotations also include: - 2,951 pairs of matching image queries and video frames - 21,412 pairs of non-matching image queries and video frames (which were verified to avoid visual similarities).
<br><br>

<a href="http://purl.stanford.edu/pj408hq3574"><b>Download Dataset</b></a>
<br/><br /><br />

<center>
    <img src="imgs/CNN_2H_Videos_Dataset.jpg" width="400" height="285"><br><br><br>
</center>

References:<br/>
<ol>
    <li>
        A. Araujo, M. Makar, V. Chandrasekhar, D. Chen, S. Tsai, H. Chen, R. Angst, and B. Girod, "Efficient video search using image queries", IEEE International Conference on Image Processing (ICIP), October 2014. [<a href="papers/ICIP2014_VideoSearch.pdf">Paper</a>]
    </li>
</ol>
<br>
<a href="#top">Back to Top</a>
<br><br>

<!-- TemplateEndEditable -->
</div>
<hr class="clearer" />
</div>

<div id="footer">
  <ul>
    <li><a href="http://isl.stanford.edu">Information Systems Laboratory</a></li>
    <li><a href="http://ee.stanford.edu">Electrical Engineering</a></li>
    <li class="last"><a href="http://www.stanford.edu">Stanford University</a></li>
  </ul>
</div>


<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-8436897-2");
pageTracker._trackPageview();
} catch(err) {}</script>

</body>
</html>

