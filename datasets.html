<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Huizhong Chen - Datasets</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">

<div class="menu-category"><b>Huizhong Chen</b></div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="CV_HuizhongChen.pdf">Resume</a></div>
<div class="menu-item"><a href="datasets.html">Datasets</a></div>


</td>
<td id="layout-content">
<div id="toptitle">
<h1>Huizhong Chen - Datasets</h1>
</div>


<a name="top"></a>
<h2>List of Data Sets</h2>

<ul>
<li><a href="#googleiodataset">Google I/O Dataset</a><br/></li>
<li><a href="#names100dataset">Names 100 Dataset</a><br/></li>
<li><a href="#clothingattributedataset">Clothing Attributes Dataset</a><br/></li>
<li><a href="#mvsdataset">Stanford Mobile Visual Search Dataset</a><br/></li>
<li><a href="#cnn2hvideosdataset">CNN 2-Hours Videos Dataset</a><br/></li>
</ul>
<br>


<a name="googleiodataset"></a>
<hr />

<h3>Google I/O Dataset</h3><br/>
The Google I/O Dataset contains slide and spoken text data crawled from 209 presentations in the Google I/O Conference (2010-2012), with 275 manually labeled ground truth relevance judgements. The dataset is particularly suitable for studying information retrieval using multi-modal data. 
<br /><br />

<a href="http://purl.stanford.edu/gc512qf7480"><b>Download Dataset</b></a>
<br/><br/>

<center>
    <img src="imgs/GoogleIO_Dataset.jpg" height="285"><br><br><br>
</center>

References:<br/>
<ol>
    <li>
        H. Chen, M. Cooper, D. Joshi, and B. Girod, "Multi-modal Language Models for Lecture Video Retrieval", ACM Multimedia (MM), October 2014. [<a href="papers/ACM_MM14_multi-modal-language.pdf">Paper</a>]
    </li>
</ol>
<br>

<a href="#top">Back to Top</a>
<br/><br/>

<a name="names100dataset"></a>
<hr />

<h3>Names 100 Dataset</h3><br/>
We present the Names 100 Dataset, which contains 80,000 unconstrained human face images, including 100 popular names and 800 images per name. The dataset can be used to study the relation between people's first names and their facial appearance, and train name classifiers which may be used for practical applications such as gender and age recognition. 
<br /><br />

<a href="http://purl.stanford.edu/tp945cq9122"><b>Download Dataset</b></a>
<br/><br/>

<center>
    <img src="imgs/Names100_Dataset.jpg" height="350"><br><br><br>
</center>

References:<br/>
<ol>
    <li>
    Huizhong Chen, Andrew Gallagher, and Bernd Girod,  
    "What's in a Name: First Names as Facial Attributes",
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2013.
         [<a href="papers/CVPR2013_NamesAsAttributes.pdf">Paper</a>]
    </li>
  <li>
  Huizhong Chen, Andrew Gallagher, and Bernd Girod,
  "The Hidden Sides of Names - Face Modeling with First Name Attributes", 
  IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014.
  [<a href="papers/PAMI_NameAttributes_Chen_Gallagher_Girod.pdf">Paper</a>]
  </li>
</ol>
<br>

<a href="#top">Back to Top</a>
<br/><br/>


<a name="clothingattributedataset"></a>
<hr />

<h3>Clothing Attributes Dataset</h3><br/>
We introduce the Clothing Attributes Dataset for promoting research in learning visual attributes for objects. The dataset contains 1856 images, with 26 ground truth clothing attributes such as "long-sleeves", "has collar", and "striped pattern". The labels were collected using Amazon Mechanical Turk.
<br /><br />

<a href="http://purl.stanford.edu/tb980qz1002"><b>Download Dataset</b></a>
<br/><br />

<center>
    <img src="imgs/Clothing_Attributes_Dataset.jpg" height="215"><br><br><br>
</center>

References:<br/>
<ol>
    <li>
        H. Chen, A. Gallagher, and B. Girod, "Describing Clothing by Semantic Attributes", European Conference on Computer Vision (ECCV), October 2012. [<a href="papers/ECCV2012_ClothingAttributes.pdf">Paper</a>]
    </li>
</ol>
<br>


<a href="#top">Back to Top</a>
<br/><br/>


<a name="mvsdataset"></a>
<hr />

<h3>Stanford Mobile Visual Search Dataset</h3><br/>
We propose the Stanford Mobile Visual Search dataset. 
The dataset contains camera-phone images of products, CDs, books, outdoor landmarks, business cards, text documents, museum paintings and video clips. 
The dataset has several key characteristics lacking in existing datasets: rigid objects, widely varying lighting conditions, perspective distortion, foreground and background clutter, realistic ground-truth reference data, and query data collected from heterogeneous low and high-end camera phones. 
We hope that the dataset will help push research forward in the field of mobile visual search.
<br /><br />

<a href="http://purl.stanford.edu/rb470rw0983"><b>Download Dataset</b></a>
<br/><br />

<table width="500" cellpadding="0" cellspacing="0" border="0">
<tr valign="middle">
<td width="100">
<center>
<img src="imgs/MVS_Dataset_Book_Reference_001.jpg" width="90" height="139" border="0" />
<br />
Reference
</center>
</td>
<td width="100">
<center>
<img src="imgs/MVS_Dataset_Book_Droid_001.jpg" width="100" height="75" border="0" />
<br />
Motorola Droid
</center>
</td>
<td width="100">
<center>
<img src="imgs/MVS_Dataset_Book_5800_001.jpg" width="100" height="75" border="0" />
<br />
Nokia 5800
</center>
</td>
<td width="100">
<center>
<img src="imgs/MVS_Dataset_Book_iPhone_001.jpg" width="100" height="75" border="0" />
<br />
Apple iPhone
</center>
</td>
<td width="100">
<center>
<img src="imgs/MVS_Dataset_Book_Canon_001.jpg" width="100" height="75" border="0" />
<br />
Canon G11
</center>
</td>
</tr>
<tr valign="middle">
<td width="100">
<center>
<img src="imgs/MVS_Dataset_DVD_Reference_003.jpg" width="120" height="120" border="0" />
<br />
Reference
</center>
</td>
<td width="100">
<center>
<img src="imgs/MVS_Dataset_DVD_Droid_003.jpg" width="100" height="75" border="0" />
<br />
Motorola Droid
</center>
</td>
<td width="100">
<center>
<img src="imgs/MVS_Dataset_DVD_Pre_003.jpg" width="100" height="75" border="0" />
<br />
Palm Pre
</center>
</td>
<td width="100">
<center>
<img src="imgs/MVS_Dataset_DVD_E63_003.jpg" width="100" height="75" border="0" />
<br />
Nokia E63
</center>
</td>
<td width="100">
<center>
<img src="imgs/MVS_Dataset_DVD_Canon_003.jpg" width="100" height="75" border="0" />
<br />
Canon G11
</center>
</td>
</tr>
<tr valign="middle">
<td width="100">
<center>
<img src="imgs/MVS_Dataset_Painting_Reference_017.jpg" width="80" height="150" border="0" />
<br />
Reference
</center>
</td>
<td width="100">
<center>
<img src="imgs/MVS_Dataset_Painting_Droid_017.jpg" width="100" height="75" border="0" />
<br />
Motorola Droid
</center>
</td>
<td width="100">
<center>
<img src="imgs/MVS_Dataset_Painting_Pre_017.jpg" width="75" height="100" border="0" />
<br />
Palm Pre
</center>
</td>
<td width="100">
<center>
<img src="imgs/MVS_Dataset_Painting_E63_017.jpg" width="100" height="75" border="0" />
<br />
Nokia E63
</center>
</td>
<td width="120">
<center>
<img src="imgs/MVS_Dataset_Painting_Canon_017.jpg" width="100" height="75" border="0" />
<br />
Canon G11
</center>
</td>
</tr>
</table>
<br /><br/>

References:<br/>
<ol>
<li>
V. Chandrasekhar, D. Chen, S. Tsai, N.-M. Cheung, H. Chen, G. Takacs, Y. Reznik, R. Vedantham, R. Grzeszczuk, J. Bach, and B. Girod, "The Stanford mobile visual search dataset", ACM Multimedia Systems Conference (MMSys), February 2011.
[<a href="papers/ACMMMSys2011_VisualSearchDataset.pdf">Paper</a>]
</li>
</ol>
<br/>

<a href="#top">Back to Top</a>
<br/><br/>


<a name="cnn2hvideosdataset"></a>
<hr />

<h3>CNN 2-Hours Videos Dataset</h3><br/>

We present the CNN2h dataset, which can be used for evaluating systems that search videos using image queries. It contains 2 hours of video and 139 image queries with annotated ground truth (based on video frames extracted at 10 frames per second). The annotations also include: - 2,951 pairs of matching image queries and video frames - 21,412 pairs of non-matching image queries and video frames (which were verified to avoid visual similarities).
<br><br>

<a href="http://purl.stanford.edu/pj408hq3574"><b>Download Dataset</b></a>
<br/><br />

<center>
    <img src="imgs/CNN_2H_Videos_Dataset.jpg" width="400" height="285"><br><br><br>
</center>

References:<br/>
<ol>
    <li>
        A. Araujo, M. Makar, V. Chandrasekhar, D. Chen, S. Tsai, H. Chen, R. Angst, and B. Girod, "Efficient video search using image queries", IEEE International Conference on Image Processing (ICIP), October 2014. [<a href="papers/ICIP2014_VideoSearch.pdf">Paper</a>]
    </li>
</ol>
<br>
<a href="#top">Back to Top</a>
<br><br>


<!--
<div id="footer">
<div id="footer-text">
Page generated 2014-03-01, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
-->
</td>
</tr>
</table>
</body>
</html>

